{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Réflexion sur l'attention\n",
    "Rélexion basée sur Attention is all you need v.7 : https://arxiv.org/abs/1706.03762 (Vaswani et al.) (+http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "\n"
   ],
   "metadata": {
    "id": "YSHSQyukKHiE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cadre\n",
    "\n",
    "L'article montre une architecture composée d'un `encodeur` et d'un `décodeur`. Prennons une traduction. L'encodeur a pour but d'apprendre une représentation dans l'espace de la source et le décodeur exécute la traduction.\n",
    "\n",
    "## L'attention\n",
    "\n",
    "Il existe 3 contextes de matrices d'attentions :\n",
    "\n",
    "- Encodeur (self) : Capture les dépendances entre les mots de la phrase source en comparant chaque mot avec les autres mots de la même phrase source. Création d'une représentation riche et contextuelle.\n",
    "\n",
    "- Décodeur (self) : A chaque étape de la génération de la phrase cible, elle compare le mot en cours de génération avec les mots précédemment générés dans la même phrase cible. Assure un alignement harmonieux.\n",
    "\n",
    "- Décodeur (cross) Combine l'information de la phrase source et de la phrase cible en cours de génération. Le modèle de \"regarde\" la phrase source pendant le processus de traduction et donc s'assure de la précision et de la cohérence dans la traduction résultante.\n",
    "\n",
    "---\n",
    "NB : L'article ne couvre pas le méchanisme de tokenization mais voici la présentation d'un outil qui le fait : https://spacy.io/api et un article fondateur au niveau de la représentation de ceux-ci dans l'espace : `Efficient Estimation of Word Representations in Vector Space` https://arxiv.org/abs/1301.3781 (Mikolov et al.)\n",
    "\n",
    "---\n",
    "\n",
    "## Scaled dot product attention\n",
    "\n",
    "$Attention(Q, K, V) = softmax( \\frac{QK^T}{\\sqrt d_k} ) V$, Où $d_k$ est la dimension de l'embedding. (2)\n",
    "\n",
    "---\n",
    "\n",
    "Définition de Q, K et V :\n",
    "\n",
    "- La *requête* (query) est le mot que nous cherchons dans la séquence.\n",
    "- La *clé* (key) indique ce qui est important en se basant sur la requête et peut\n",
    "être vue comme une indexation unique de la valeur.\n",
    "- La *valeur* (value) est l'information contenue dans le mot d'entrée, elle contribue à former la nouvelle représentation du mot en cours de génération.\n",
    "- La *fonction de score* prends la requête et la clé comme entrée et sa sortie est le poids des deux (la similarité entre les deux, ce sur quoi l'attention se porte)\n"
   ],
   "metadata": {
    "id": "svI-ksnGLOa0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exemple\n",
    "\n",
    "Prennons l'exemple d'une traduction. Disons que nous traduisons :\n",
    "1. La branche au soleil se dore et penche pour l'abriter | $t-1$\n",
    "2. ses boutons qui vont éclore sur l'oiseau qui va chanter | $t$\n",
    "\n",
    "Une traduction anglaise serait :\n",
    "1. The branch tans in the sun and lean to shelter | $t-1$\n",
    "2. her pimples that are about to hatch on the bird that will sing | $t$\n",
    "\n",
    "---\n",
    "\n",
    "Dans attention is all you need (https://arxiv.org/abs/1706.03762) Q, K et V sont la même matrice (en pratique un tenseur avec les batchs) mais il y a aussi deux éléments intéressants dans l'architecture. C'est le shifted right en entrée du décodeur et le fait que tout soit traité en parrallèle (ce qui justifie le posisional encoding). Alors pour illustrer les concepts de multiplication matricielle, réfléchissons en décallant tout cela.\n",
    "\n",
    "Imaginons que nous sommes au temps $t$.\n",
    "\n",
    "Au niveau de *l'auto-attention* de l'encodeur, nous avons :\n",
    "\n",
    "- Q = \"*ses boutons qui vont éclore sur l'oiseau qui va chanter*\" (sous la forme d'une matrice)\n",
    "- K = \"*La branche au soleil se dore et penche pour l'abriter*\" (sous la forme d'une matrice)\n",
    "- V = \"*ses*\" (sous la forme d'un vecteur dense)\n",
    "\n",
    "Au niveau de *l'auto-attention* du décodeur, nous avons :\n",
    "\n",
    "- Q = \"*her pimples that are about to hatch on the bird that will sing*\" (sous la forme d'une matrice)\n",
    "- K = \"*The branch tans in the sun and lean to shelter*\" (sous la forme d'une matrice)\n",
    "- V = \"*her*\" (sous la forme d'un vecteur dense)\n",
    "\n",
    "Au niveau de *l'attention croisée* du décodeur, nous avons :\n",
    "\n",
    "\n",
    "- Q = \"*ses boutons qui vont éclore sur l'oiseau qui va chanter*\" (sous la forme d'une matrice)\n",
    "- K = \"*The branch tans in the sun and lean to shelter*\" (sous la forme d'une matrice)\n",
    "- V = \"*her*\" (sous la forme d'un vecteur dense)\n",
    "\n",
    "Sachant que les valeurs de V vaudront tour à tour chaque mot de la phrase de la requête et que tout cela est en pratique traité en parrallèle, ce que nous déconstruisons pour clarifier l'expliation.\n"
   ],
   "metadata": {
    "id": "_aUhCs2dJiCw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy\n",
    "import math\n",
    "import warnings\n",
    "from scipy.special import softmax\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rand=random.Random()\n",
    "dim = 512\n",
    "min = 0.000001\n",
    "max = 1\n",
    "\n",
    "def generate_embedding():\n",
    "  lst = []\n",
    "  for i in range(dim):\n",
    "    lst.append(rand.uniform(min, max))\n",
    "\n",
    "  return lst\n"
   ],
   "metadata": {
    "id": "cGiEwMVHB8BX"
   },
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculons un score d'attention :\n",
    "\n",
    "Score d'attention = $Q \\cdot K^T$ (1)\n",
    "\n",
    "Reproduisons, sur base de l'exemple ..\n",
    "\n",
    "- Q, une matrice $11 \\times 512$ pour \"ses boutons qui vont éclore sur l'oiseau qui va chanter\"\n",
    "- K, une matrice $10 \\times 512$ pour \"The branch tans in the sun and lean to shelter\"\n",
    "- V, un vecteur $1 \\times 512$ pour \"her\"\n",
    "\n",
    "Ce score d'attention résulte du produit d'une matrice $11 \\times 512$ pour la source (query) et d'une matrice $10 \\times 512$ pour la cible (key). Appliquons (1) pour obtenir une matrice $11 \\times 10$ :"
   ],
   "metadata": {
    "id": "3zRSnFT5B_nJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Q = [generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding()]\n",
    "K = [generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding(), generate_embedding()]\n",
    "V = generate_embedding()\n",
    "\n",
    "# Affichage des dimensions\n",
    "print(\"Q : ses boutons qui vont éclore sur l'oiseau qui va chanter : \", numpy.shape(Q))\n",
    "print(\"K : The branch tans in the sun and lean to shelter : \", numpy.shape(K))\n",
    "print()\n",
    "\n",
    "# Transposition\n",
    "K_T = (numpy.array(K).T).tolist()\n",
    "\n",
    "# Calcul d'un score d'attention\n",
    "attention_score = numpy.dot(Q, K_T)\n",
    "\n",
    "# Affichage score d'attention\n",
    "# print(\"score d'attention : \", attention_score)\n",
    "print(\"dimension : \", numpy.shape(attention_score), \"c'est à dire Q . K^T\")\n",
    "print()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnJeX_cdcGd4",
    "outputId": "ed8ac0d8-7664-4dc7-e26e-5eba81f37cba"
   },
   "execution_count": 75,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q : ses boutons qui vont éclore sur l'oiseau qui va chanter :  (11, 512)\n",
      "K : The branch tans in the sun and lean to shelter :  (10, 512)\n",
      "\n",
      "dimension :  (11, 10) c'est à dire Q . K^T\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité\n",
    "Au plus les valeurs du score d'attention sont élevés au plus il y a de la similarité entre les membres du produit matriciel par définition.\n",
    "\n",
    "- Documentation géométrique : https://fr.wikipedia.org/wiki/Similitude_(g%C3%A9om%C3%A9trie)\n",
    "\n",
    "Si la similarité est forte entre les deux matrices on dit que le modèle porte attention. Voici la tête de cette matrice dont la représentation est également générée aléatoirement :\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "d88DXzppawWj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Nombre de mots dans la séquence cible (Q) et la séquence source (K)\n",
    "num_words_q = len(Q)\n",
    "num_words_k = len(K)\n",
    "\n",
    "# Génération de scores de similarité aléatoires\n",
    "scores = numpy.random.rand(num_words_q, num_words_k)\n",
    "\n",
    "# Affichage de la matrice de scores\n",
    "for i in range(num_words_q):\n",
    "    if i > 9:\n",
    "      num = str(i)\n",
    "    else:\n",
    "      num = \"0\"+str(i)\n",
    "\n",
    "    print(\"mot{} (Q) |\".format(num), end=\" \")\n",
    "    for j in range(num_words_k):\n",
    "        print(\"{:.2f}\".format(scores[i][j]), end=\"    \")\n",
    "    print()\n",
    "\n",
    "print(\"         --------------------------------------------------------------------------------\")\n",
    "for j in range(num_words_k):\n",
    "  if j == 0:\n",
    "    print(\"            mot{}\".format(j + 1), end=\"   \")\n",
    "  else:\n",
    "    print(\" mot{}\".format(j + 1), end=\"   \")\n",
    "print()\n",
    "\n",
    "print(\"            Mots dans la séquence source (K)\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpeU98iU8CQh",
    "outputId": "61e4cd5d-390c-4aab-f1f9-bdf690c7b021"
   },
   "execution_count": 67,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mot00 (Q) | 0.67    0.87    0.97    0.28    0.66    0.74    0.80    0.40    0.03    0.34    \n",
      "mot01 (Q) | 0.27    0.08    0.69    0.30    0.12    0.61    0.20    0.68    0.95    0.57    \n",
      "mot02 (Q) | 0.69    0.19    0.19    0.47    0.23    0.24    0.75    0.19    0.72    0.25    \n",
      "mot03 (Q) | 0.22    0.87    0.23    0.17    0.16    0.62    0.50    0.71    0.36    0.53    \n",
      "mot04 (Q) | 0.11    0.99    0.14    0.39    0.04    0.97    0.65    0.74    0.09    0.77    \n",
      "mot05 (Q) | 0.81    0.84    0.71    0.47    0.58    0.55    0.58    0.78    0.26    0.49    \n",
      "mot06 (Q) | 0.15    0.88    0.45    0.41    0.77    0.00    0.67    0.64    0.75    0.17    \n",
      "mot07 (Q) | 0.06    0.19    0.63    0.01    0.22    0.12    0.02    0.28    0.38    0.54    \n",
      "mot08 (Q) | 0.54    0.74    0.49    0.78    0.12    0.79    0.92    0.90    0.04    0.56    \n",
      "mot09 (Q) | 0.47    0.28    0.92    0.94    0.68    0.25    0.65    0.27    0.94    0.58    \n",
      "mot10 (Q) | 0.08    0.26    0.39    0.46    0.92    0.56    0.31    0.63    0.16    0.43    \n",
      "         --------------------------------------------------------------------------------\n",
      "            mot1    mot2    mot3    mot4    mot5    mot6    mot7    mot8    mot9    mot10   \n",
      "            Mots dans la séquence source (K)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calcul de l'attention\n",
    "\n",
    "Ce qui suit consiste en\n",
    "- une normalisation (mise à l'échelle) des scores\n",
    "- une softmax qui transforme la matrice en un vecteur de probabilté\n",
    "- un produit scalaire avec V"
   ],
   "metadata": {
    "id": "-t-JB-xwxG8M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# initialization\n",
    "s = []\n",
    "\n",
    "# Mise à l'échelle\n",
    "normalized_attention_score = attention_score / (math.sqrt(dim))\n",
    "print(\"score d'attention normalisée : \")\n",
    "print(\"------------------------------\")\n",
    "\n",
    "# Affichage de la matrice de scores\n",
    "for i in range(num_words_q):\n",
    "    if i > 9:\n",
    "      num = str(i)\n",
    "    else:\n",
    "      num = \"0\"+str(i)\n",
    "\n",
    "    print(\"mot{} (Q) |\".format(num), end=\" \")\n",
    "    s_i = 0\n",
    "    for j in range(num_words_k):\n",
    "      s_i += normalized_attention_score[i][j]\n",
    "      print(\"{:.2f}\".format(normalized_attention_score[i][j]), end=\"    \")\n",
    "\n",
    "    s.append(s_i)\n",
    "    print()\n",
    "\n",
    "print(\"         --------------------------------------------------------------------------------\")\n",
    "for j in range(num_words_k):\n",
    "  if j == 0:\n",
    "    print(\"            mot{}\".format(j + 1), end=\"   \")\n",
    "  else:\n",
    "    print(\" mot{}\".format(j + 1), end=\"   \")\n",
    "print()\n",
    "\n",
    "print(\"            Mots dans la séquence source (K)\")\n",
    "print()\n",
    "\n",
    "# Softmax\n",
    "s_att = softmax(s)\n",
    "print(\"probabilité d'attention : \", s_att)\n",
    "print()\n",
    "\n",
    "# Scaled dot product attention\n",
    "s_att = torch.tensor(s_att)\n",
    "V = torch.tensor(V)\n",
    "\n",
    "att = torch.sum(s_att.unsqueeze(-1) * V, dim=0)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"dimensions des résultats : \")\n",
    "print(\"--------------------------\")\n",
    "print()\n",
    "\n",
    "print(\"s_att : \", numpy.shape(s_att))\n",
    "print(\"V : \", numpy.shape(V))\n",
    "print(\"dot product attention pour un mot : \", numpy.shape(att))\n"
   ],
   "metadata": {
    "id": "9Byyt072Rgfc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7e047c72-3133-41e4-a752-206c1d0d4f5e"
   },
   "execution_count": 91,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "score d'attention normalisée : \n",
      "------------------------------\n",
      "mot00 (Q) | 5.51    5.61    5.72    5.46    5.74    5.48    5.04    5.55    5.31    5.24    \n",
      "mot01 (Q) | 5.74    5.76    5.69    5.70    5.82    5.61    5.25    5.55    5.38    5.46    \n",
      "mot02 (Q) | 5.89    5.87    5.75    5.66    6.05    5.72    5.51    5.79    5.55    5.64    \n",
      "mot03 (Q) | 5.79    5.86    5.73    5.79    5.99    5.86    5.39    5.80    5.44    5.71    \n",
      "mot04 (Q) | 5.79    5.76    5.91    5.79    5.86    5.88    5.33    5.83    5.44    5.54    \n",
      "mot05 (Q) | 5.86    5.93    5.87    5.90    5.98    5.76    5.33    5.92    5.57    5.62    \n",
      "mot06 (Q) | 5.74    6.06    5.89    6.04    6.24    6.03    5.44    5.97    5.73    5.68    \n",
      "mot07 (Q) | 5.77    5.88    6.01    5.97    5.99    5.73    5.39    5.78    5.58    5.71    \n",
      "mot08 (Q) | 5.52    5.74    5.73    5.47    5.95    5.63    5.22    5.49    5.28    5.36    \n",
      "mot09 (Q) | 5.49    5.62    5.73    5.47    5.64    5.52    5.26    5.47    5.38    5.25    \n",
      "mot10 (Q) | 6.01    5.96    5.96    5.84    6.06    5.99    5.39    5.86    5.77    5.70    \n",
      "         --------------------------------------------------------------------------------\n",
      "            mot1    mot2    mot3    mot4    mot5    mot6    mot7    mot8    mot9    mot10   \n",
      "            Mots dans la séquence source (K)\n",
      "\n",
      "probabilité d'attention :  [0.00487974 0.01798049 0.07682518 0.07047701 0.05622309 0.10626933\n",
      " 0.30908868 0.11183344 0.00997815 0.00573745 0.23070744]\n",
      "\n",
      "\n",
      "dimensions des résultats : \n",
      "--------------------------\n",
      "\n",
      "s_att :  torch.Size([11])\n",
      "V :  torch.Size([512])\n",
      "dot product attention pour un mot :  torch.Size([512])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous avons traité le premier mot, en parrallèle sont traités chaque mots, dans l'exemple nous en avons $12$, ce qui donne une matrice $11 \\times 12$ à laquelle nous ajoutons l'embedding c'est à dire un tenseur $11 \\times 12 \\times 512$ lorsque chaque mot est traité."
   ],
   "metadata": {
    "id": "OW-NZS3RYYVC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercice supplémentaire\n",
    "\n",
    "Effectuer cet exercice avec de vrais embeddings en utilisant https://fasttext.cc/"
   ],
   "metadata": {
    "id": "Od7krlV-Y7I2"
   }
  }
 ]
}
